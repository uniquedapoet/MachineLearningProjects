{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/anaconda3/lib/python3.11/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from SimulateDay import stock_market_simulation, scale_data,train_Optimal_Action, _select_stock, get_stock_data, add_columns, train_Optimal_Action\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def importModels(symbol):\n",
    "    specificModel = joblib.load(f\"models/{symbol}_model.pkl\")\n",
    "    noCovidModel = train_Optimal_Action(symbol=symbol, action_column='Action')\n",
    "    generalModel = xgb.Booster()    \n",
    "    generalModel.load_model('models/all_stocks_incremental_model.pkl') \n",
    "    return specificModel, generalModel, noCovidModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for TSLA ...\n",
      "Adding columns for TSLA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:238: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n",
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:238: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m specificModel, generalModel, noCovidModel \u001b[38;5;241m=\u001b[39m importModels(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTSLA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m, in \u001b[0;36mimportModels\u001b[0;34m(symbol)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimportModels\u001b[39m(symbol):\n\u001b[1;32m     15\u001b[0m     specificModel \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     noCovidModel \u001b[38;5;241m=\u001b[39m train_Optimal_Action(symbol\u001b[38;5;241m=\u001b[39msymbol, action_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     generalModel \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mBooster()    \n\u001b[1;32m     18\u001b[0m     generalModel\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/all_stocks_incremental_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[0;32m~/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:546\u001b[0m, in \u001b[0;36mtrain_Optimal_Action\u001b[0;34m(symbol, action_column)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdding columns for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    545\u001b[0m stock_df \u001b[38;5;241m=\u001b[39m add_columns(stock_df)\n\u001b[0;32m--> 546\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m scale_data(stock_df)\n\u001b[1;32m    547\u001b[0m X \u001b[38;5;241m=\u001b[39m preprocessed[features]\n\u001b[1;32m    548\u001b[0m y \u001b[38;5;241m=\u001b[39m stock_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:453\u001b[0m, in \u001b[0;36mscale_data\u001b[0;34m(stock_df)\u001b[0m\n\u001b[1;32m    451\u001b[0m min_max_scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m    452\u001b[0m stock_df \u001b[38;5;241m=\u001b[39m add_columns(stock_df)\n\u001b[0;32m--> 453\u001b[0m stock_df[features] \u001b[38;5;241m=\u001b[39m min_max_scaler\u001b[38;5;241m.\u001b[39mfit_transform(stock_df[features])\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stock_df[features]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    491\u001b[0m     X,\n\u001b[1;32m    492\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_pass,\n\u001b[1;32m    493\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m_array_api\u001b[38;5;241m.\u001b[39msupported_float_dtypes(xp),\n\u001b[1;32m    494\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    495\u001b[0m )\n\u001b[1;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1069\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[1;32m    124\u001b[0m     X,\n\u001b[1;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[1;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[1;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[1;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m    130\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "\n",
    "specificModel, generalModel, noCovidModel = importModels('TSLA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Volume', 'MA_10', 'MA_20', 'MA_50', 'MA_200', 'std_10',\n",
    "                'std_20', 'std_50', 'std_200', 'upper_band_10', 'lower_band_10',\n",
    "                'upper_band_20', 'lower_band_20', 'upper_band_50', 'lower_band_50',\n",
    "                'upper_band_200', 'lower_band_200', 'Golden_Cross_Short', 'Golden_Cross_Medium',\n",
    "                'Golden_Cross_Long', 'Death_Cross_Short', 'Death_Cross_Medium', 'Death_Cross_Long',\n",
    "                'ROC', 'AVG_Volume_10', 'AVG_Volume_20', 'AVG_Volume_50', 'AVG_Volume_200', 'Doji',\n",
    "                'Bullish_Engulfing', 'Bearish_Engulfing', 'MACD', 'Signal', 'MACD_Hist', 'TR', 'ATR',\n",
    "                'RSI_10_Day', '10_Day_ROC', 'Resistance_10_Day', 'Support_10_Day', 'Resistance_20_Day',\n",
    "                'Support_20_Day', 'Resistance_50_Day', 'Support_50_Day', 'Volume_MA_10', 'Volume_MA_20',\n",
    "                'Volume_MA_50', 'OBV', 'Z-score',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_models_basic(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"SVM\": SVC(random_state=42),\n",
    "        \"XGBoost\": xgb.XGBClassifier(n_estimators=100, random_state=42),\n",
    "        \"Stacking\": StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "                ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42)),\n",
    "            ],\n",
    "            final_estimator=xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        results[name] = {\n",
    "            \"Model\": model,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Classification Report\": report\n",
    "        }\n",
    "        \n",
    "        print(f\"--- {name} Results ---\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return results\n",
    "def train_classification_model_tuned(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        \"Logistic Regression Tuned\": LogisticRegression(C=1.0, penalty='l2', solver='saga', max_iter=1000, random_state=42),\n",
    "        \"Random Forest Tuned\": RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_split=10, min_samples_leaf=5, max_features='sqrt', random_state=42),\n",
    "        \"SVM Tuned \": SVC(random_state=42, C=10.0, kernel='rbf',gamma='scale'),\n",
    "        \"XGBoost Tuned\": xgb.XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, gamma=0.1, random_state=42),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        results[name] = {\n",
    "            \"Model\": model,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Classification Report\": report\n",
    "        }\n",
    "        \n",
    "        print(f\"--- {name} Results ---\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulations_on_models(X_train, y_train, X_test, y_test, stock_data, initial_cash, days):\n",
    "    # Train different models\n",
    "    trained_models = train_classification_models_basic(X_train, y_train, X_test, y_test)\n",
    "    # trained_models_tuned = train_classification_models_basic(X_train, y_train, X_test, y_test)\n",
    "    # trained_models = {**trained_models_basic, **trained_models_tuned}    \n",
    "    # # Dictionary to store simulation results for each model\n",
    "    simulation_results = {}\n",
    "    \n",
    "    # Run stock market simulation for each trained model\n",
    "    for model_name, model_data in trained_models.items():\n",
    "        model = model_data['Model']\n",
    "        print(f\"Running stock market simulation for {model_name}...\")\n",
    "        \n",
    "        # Run the simulation\n",
    "        simulation_df, final_cash = stock_market_simulation(model, initial_cash, days, stock_data)\n",
    "        \n",
    "        # Store the simulation results\n",
    "        simulation_results[model_name] = simulation_df\n",
    "        \n",
    "    return simulation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:238: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# stock_df = get_stock_data('TSLA')\n",
    "# print(f'Adding columns...')\n",
    "stock_df = pd.read_csv('data/sp500_stocks.csv')\n",
    "stock_df = stock_df[stock_df['Symbol'] == 'TSLA']\n",
    "print(f'Scaling data...')\n",
    "preprocessed = scale_data(stock_df)\n",
    "X = preprocessed[features]\n",
    "y = stock_df['Action']\n",
    "# y = y.map({'Buy': 0, 'Sell': 1, 'Hold': 2})\n",
    "X_train, _, y_train, _ = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression Results ---\n",
      "Accuracy: 0.8644922663080027\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86      1045\n",
      "           1       0.84      0.84      0.84       797\n",
      "           2       0.90      0.87      0.89      1132\n",
      "\n",
      "    accuracy                           0.86      2974\n",
      "   macro avg       0.86      0.86      0.86      2974\n",
      "weighted avg       0.87      0.86      0.86      2974\n",
      "\n",
      "Training Random Forest...\n",
      "--- Random Forest Results ---\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1045\n",
      "           1       1.00      1.00      1.00       797\n",
      "           2       1.00      1.00      1.00      1132\n",
      "\n",
      "    accuracy                           1.00      2974\n",
      "   macro avg       1.00      1.00      1.00      2974\n",
      "weighted avg       1.00      1.00      1.00      2974\n",
      "\n",
      "Training SVM...\n",
      "--- SVM Results ---\n",
      "Accuracy: 0.8375924680564896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84      1045\n",
      "           1       0.80      0.87      0.83       797\n",
      "           2       0.94      0.76      0.84      1132\n",
      "\n",
      "    accuracy                           0.84      2974\n",
      "   macro avg       0.84      0.84      0.84      2974\n",
      "weighted avg       0.85      0.84      0.84      2974\n",
      "\n",
      "Training XGBoost...\n",
      "--- XGBoost Results ---\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1045\n",
      "           1       1.00      1.00      1.00       797\n",
      "           2       1.00      1.00      1.00      1132\n",
      "\n",
      "    accuracy                           1.00      2974\n",
      "   macro avg       1.00      1.00      1.00      2974\n",
      "weighted avg       1.00      1.00      1.00      2974\n",
      "\n",
      "Training Stacking...\n",
      "--- Stacking Results ---\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1045\n",
      "           1       1.00      1.00      1.00       797\n",
      "           2       1.00      1.00      1.00      1132\n",
      "\n",
      "    accuracy                           1.00      2974\n",
      "   macro avg       1.00      1.00      1.00      2974\n",
      "weighted avg       1.00      1.00      1.00      2974\n",
      "\n",
      "Running stock market simulation for Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:238: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n",
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:143: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  modelDecisionDf = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stock market simulation for Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:238: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n",
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:143: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  modelDecisionDf = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stock market simulation for SVM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:238: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n",
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:143: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  modelDecisionDf = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stock market simulation for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:238: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n",
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:143: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  modelDecisionDf = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stock market simulation for Stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:238: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n",
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:143: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  modelDecisionDf = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "simResults = run_simulations_on_models(X_train, y_train, X_train, y_train,get_stock_data('TSLA').tail(365) , 10000, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:238: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n",
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:143: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  modelDecisionDf = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 30: Bought 1 share at 256.7900085449219, Cash left: 9743.209991455078\n",
      "Day 34: Sold 1 share at 259.4599914550781, Cash: 10002.669982910156\n",
      "Day 83: Bought 1 share at 256.8999938964844, Cash left: 9745.769989013672\n",
      "Day 84: Sold 1 share at 258.0799865722656, Cash: 10003.849975585938\n",
      "Day 85: Bought 1 share at 245.0099945068359, Cash left: 9758.839981079102\n",
      "Day 87: Bought 1 share at 251.9199981689453, Cash left: 9506.919982910156\n",
      "Day 88: Sold 1 share at 251.4900054931641, Cash: 9758.40998840332\n",
      "Day 89: Sold 1 share at 248.5, Cash: 10006.90998840332\n",
      "Day 91: Bought 1 share at 267.4800109863281, Cash left: 9739.429977416992\n",
      "Day 94: Sold 1 share at 274.3900146484375, Cash: 10013.81999206543\n",
      "Day 111: Bought 1 share at 263.6199951171875, Cash left: 9750.199996948242\n",
      "Day 112: Bought 1 share at 262.989990234375, Cash left: 9487.210006713867\n",
      "Day 113: Bought 1 share at 258.8699951171875, Cash left: 9228.34001159668\n",
      "Day 114: Sold 1 share at 251.1199951171875, Cash: 9479.460006713867\n",
      "Day 115: Sold 1 share at 253.9199981689453, Cash: 9733.380004882812\n",
      "Day 116: Sold 1 share at 254.8500061035156, Cash: 9988.230010986328\n",
      "Day 131: Bought 1 share at 222.17999267578125, Cash left: 9766.050018310547\n",
      "Day 132: Bought 1 share at 222.11000061035156, Cash left: 9543.940017700195\n",
      "Day 133: Bought 1 share at 209.97999572753903, Cash left: 9333.960021972656\n",
      "Day 134: Bought 1 share at 214.6499938964844, Cash left: 9119.310028076172\n",
      "Day 138: Bought 1 share at 233.58999633789065, Cash left: 8885.720031738281\n",
      "Day 139: Bought 1 share at 234.3000030517578, Cash left: 8651.420028686523\n",
      "Day 140: Bought 1 share at 235.6000061035156, Cash left: 8415.820022583008\n",
      "Day 141: Bought 1 share at 241.1999969482422, Cash left: 8174.620025634766\n",
      "Day 142: Bought 1 share at 234.2100067138672, Cash left: 7940.410018920898\n",
      "Day 143: Bought 1 share at 235.4499969482422, Cash left: 7704.960021972656\n",
      "Day 144: Bought 1 share at 236.0800018310547, Cash left: 7468.880020141602\n",
      "Day 146: Bought 1 share at 244.13999938964844, Cash left: 7224.740020751953\n",
      "Day 147: Bought 1 share at 240.0800018310547, Cash left: 6984.660018920898\n",
      "Day 148: Bought 1 share at 238.8300018310547, Cash left: 6745.830017089844\n",
      "Day 149: Bought 1 share at 235.5800018310547, Cash left: 6510.250015258789\n",
      "Day 150: Sold 1 share at 238.72000122070312, Cash: 6748.970016479492\n",
      "Day 151: Sold 1 share at 239.3699951171875, Cash: 6988.34001159668\n",
      "Day 152: Sold 1 share at 242.63999938964844, Cash: 7230.980010986328\n",
      "Day 153: Sold 1 share at 243.83999633789065, Cash: 7474.820007324219\n",
      "Day 154: Sold 1 share at 239.7400054931641, Cash: 7714.560012817383\n",
      "Day 155: Sold 1 share at 237.0099945068359, Cash: 7951.570007324219\n",
      "Day 156: Sold 1 share at 239.2899932861328, Cash: 8190.860000610352\n",
      "Day 157: Sold 1 share at 251.0500030517578, Cash: 8441.91000366211\n",
      "Day 158: Bought 1 share at 253.5, Cash left: 8188.410003662109\n",
      "Day 159: Bought 1 share at 252.0800018310547, Cash left: 7936.330001831055\n",
      "Day 161: Sold 1 share at 247.13999938964844, Cash: 8183.470001220703\n",
      "Day 162: Sold 1 share at 254.5, Cash: 8437.970001220703\n",
      "Day 163: Sold 1 share at 252.5399932861328, Cash: 8690.509994506836\n",
      "Day 164: Bought 1 share at 256.6099853515625, Cash left: 8433.900009155273\n",
      "Day 166: Sold 1 share at 253.17999267578125, Cash: 8687.080001831055\n",
      "Day 167: Sold 1 share at 248.47999572753903, Cash: 8935.559997558594\n",
      "Day 168: Sold 1 share at 248.4199981689453, Cash: 9183.979995727539\n",
      "Day 169: Sold 1 share at 238.4499969482422, Cash: 9422.429992675781\n",
      "Day 170: Sold 1 share at 237.92999267578125, Cash: 9660.359985351562\n",
      "Day 171: Sold 1 share at 237.4900054931641, Cash: 9897.849990844727\n",
      "Day 172: Sold 1 share at 240.4499969482422, Cash: 10138.299987792969\n",
      "Day 195: Bought 1 share at 193.57000732421875, Cash left: 9944.72998046875\n",
      "Day 196: Bought 1 share at 188.1300048828125, Cash left: 9756.599975585938\n",
      "Day 197: Bought 1 share at 184.02000427246097, Cash left: 9572.579971313477\n",
      "Day 198: Bought 1 share at 188.7100067138672, Cash left: 9383.86996459961\n",
      "Day 200: Bought 1 share at 199.9499969482422, Cash left: 9183.919967651367\n",
      "Day 201: Bought 1 share at 193.7599945068359, Cash left: 8990.159973144531\n",
      "Day 202: Bought 1 share at 194.77000427246097, Cash left: 8795.38996887207\n",
      "Day 203: Bought 1 share at 197.41000366210935, Cash left: 8597.979965209961\n",
      "Day 204: Bought 1 share at 191.97000122070312, Cash left: 8406.009963989258\n",
      "Day 205: Bought 1 share at 199.3999938964844, Cash left: 8206.609970092773\n",
      "Day 206: Bought 1 share at 199.72999572753903, Cash left: 8006.879974365234\n",
      "Day 207: Bought 1 share at 202.0399932861328, Cash left: 7804.839981079102\n",
      "Day 208: Bought 1 share at 201.8800048828125, Cash left: 7602.959976196289\n",
      "Day 209: Bought 1 share at 202.63999938964844, Cash left: 7400.319976806641\n",
      "Day 210: Bought 1 share at 188.13999938964844, Cash left: 7212.179977416992\n",
      "Day 211: Bought 1 share at 180.7400054931641, Cash left: 7031.439971923828\n",
      "Day 212: Sold 1 share at 176.5399932861328, Cash: 7207.979965209961\n",
      "Day 214: Sold 1 share at 175.33999633789062, Cash: 7383.319961547852\n",
      "Day 216: Sold 1 share at 177.5399932861328, Cash: 7560.859954833984\n",
      "Day 217: Sold 1 share at 169.47999572753906, Cash: 7730.339950561523\n",
      "Day 218: Sold 1 share at 162.5, Cash: 7892.839950561523\n",
      "Day 219: Sold 1 share at 163.57000732421875, Cash: 8056.409957885742\n",
      "Day 221: Sold 1 share at 171.32000732421875, Cash: 8227.729965209961\n",
      "Day 222: Sold 1 share at 175.66000366210938, Cash: 8403.38996887207\n",
      "Day 223: Bought 1 share at 172.82000732421875, Cash left: 8230.569961547852\n",
      "Day 224: Sold 1 share at 170.8300018310547, Cash: 8401.399963378906\n",
      "Day 225: Sold 1 share at 172.6300048828125, Cash: 8574.029968261719\n",
      "Day 227: Bought 1 share at 179.8300018310547, Cash left: 8394.199966430664\n",
      "Day 228: Bought 1 share at 175.7899932861328, Cash left: 8218.409973144531\n",
      "Day 229: Bought 1 share at 175.22000122070312, Cash left: 8043.189971923828\n",
      "Day 230: Bought 1 share at 166.6300048828125, Cash left: 7876.559967041016\n",
      "Day 231: Sold 1 share at 168.3800048828125, Cash: 8044.939971923828\n",
      "Day 232: Sold 1 share at 171.11000061035156, Cash: 8216.04997253418\n",
      "Day 233: Bought 1 share at 164.89999389648438, Cash left: 8051.149978637695\n",
      "Day 235: Sold 1 share at 176.8800048828125, Cash: 8228.029983520508\n",
      "Day 236: Bought 1 share at 171.75999450683594, Cash left: 8056.269989013672\n",
      "Day 237: Sold 1 share at 174.60000610351562, Cash: 8230.869995117188\n",
      "Day 238: Bought 1 share at 171.0500030517578, Cash left: 8059.81999206543\n",
      "Day 239: Sold 1 share at 161.47999572753906, Cash: 8221.299987792969\n",
      "Day 240: Sold 1 share at 157.11000061035156, Cash: 8378.40998840332\n",
      "Day 241: Sold 1 share at 155.4499969482422, Cash: 8533.859985351562\n",
      "Day 242: Sold 1 share at 149.92999267578125, Cash: 8683.789978027344\n",
      "Day 243: Sold 1 share at 147.0500030517578, Cash: 8830.839981079102\n",
      "Day 244: Sold 1 share at 142.0500030517578, Cash: 8972.88998413086\n",
      "Day 245: Sold 1 share at 144.67999267578125, Cash: 9117.56997680664\n",
      "Day 248: Bought 1 share at 168.2899932861328, Cash left: 8949.279983520508\n",
      "Day 250: Bought 1 share at 183.27999877929688, Cash left: 8765.999984741211\n",
      "Day 251: Bought 1 share at 179.99000549316406, Cash left: 8586.009979248047\n",
      "Day 252: Bought 1 share at 180.00999450683594, Cash left: 8405.999984741211\n",
      "Day 253: Bought 1 share at 181.19000244140625, Cash left: 8224.809982299805\n",
      "Day 254: Bought 1 share at 184.7599945068359, Cash left: 8040.049987792969\n",
      "Day 255: Bought 1 share at 177.80999755859375, Cash left: 7862.239990234375\n",
      "Day 256: Bought 1 share at 174.72000122070312, Cash left: 7687.519989013672\n",
      "Day 257: Bought 1 share at 171.97000122070312, Cash left: 7515.549987792969\n",
      "Day 258: Bought 1 share at 168.47000122070312, Cash left: 7347.079986572266\n",
      "Day 260: Sold 1 share at 177.5500030517578, Cash: 7524.629989624023\n",
      "Day 261: Bought 1 share at 173.99000549316406, Cash left: 7350.639984130859\n",
      "Day 263: Sold 1 share at 177.4600067138672, Cash: 7528.099990844727\n",
      "Day 264: Sold 1 share at 174.9499969482422, Cash: 7703.049987792969\n",
      "Day 266: Bought 1 share at 180.11000061035156, Cash left: 7522.939987182617\n",
      "Day 267: Sold 1 share at 173.74000549316406, Cash: 7696.679992675781\n",
      "Day 269: Bought 1 share at 176.75, Cash left: 7519.929992675781\n",
      "Day 270: Sold 1 share at 176.19000244140625, Cash: 7696.1199951171875\n",
      "Day 271: Sold 1 share at 178.7899932861328, Cash: 7874.90998840332\n",
      "Day 272: Sold 1 share at 178.0800018310547, Cash: 8052.989990234375\n",
      "Day 273: Sold 1 share at 176.2899932861328, Cash: 8229.279983520508\n",
      "Day 274: Bought 1 share at 174.77000427246094, Cash left: 8054.509979248047\n",
      "Day 275: Sold 1 share at 175.0, Cash: 8229.509979248047\n",
      "Day 276: Sold 1 share at 177.94000244140625, Cash: 8407.449981689453\n",
      "Day 277: Sold 1 share at 177.47999572753906, Cash: 8584.929977416992\n",
      "Day 278: Sold 1 share at 173.7899932861328, Cash: 8758.719970703125\n",
      "Day 279: Sold 1 share at 170.66000366210938, Cash: 8929.379974365234\n",
      "Day 280: Sold 1 share at 177.2899932861328, Cash: 9106.669967651367\n",
      "Day 281: Sold 1 share at 182.47000122070312, Cash: 9289.13996887207\n",
      "Day 282: Sold 1 share at 178.00999450683594, Cash: 9467.149963378906\n",
      "Day 283: Sold 1 share at 187.44000244140625, Cash: 9654.589965820312\n",
      "Day 284: Bought 1 share at 184.86000061035156, Cash left: 9469.729965209961\n",
      "Day 285: Sold 1 share at 181.57000732421875, Cash: 9651.29997253418\n",
      "Day 286: Bought 1 share at 183.0099945068359, Cash left: 9468.289978027344\n",
      "Day 287: Sold 1 share at 182.5800018310547, Cash: 9650.869979858398\n",
      "Day 289: Bought 1 share at 196.3699951171875, Cash left: 9454.499984741211\n",
      "Day 290: Bought 1 share at 197.4199981689453, Cash left: 9257.079986572266\n",
      "Day 291: Bought 1 share at 197.8800048828125, Cash left: 9059.199981689453\n",
      "Day 292: Bought 1 share at 209.86000061035156, Cash left: 8849.339981079102\n",
      "Day 293: Bought 1 share at 231.2599945068359, Cash left: 8618.079986572266\n",
      "Day 294: Bought 1 share at 246.38999938964844, Cash left: 8371.689987182617\n",
      "Day 295: Bought 1 share at 251.52000427246097, Cash left: 8120.169982910156\n",
      "Day 296: Bought 1 share at 252.94000244140625, Cash left: 7867.22998046875\n",
      "Day 297: Bought 1 share at 262.3299865722656, Cash left: 7604.899993896484\n",
      "Day 298: Bought 1 share at 263.260009765625, Cash left: 7341.639984130859\n",
      "Day 299: Bought 1 share at 241.02999877929688, Cash left: 7100.6099853515625\n",
      "Day 301: Bought 1 share at 252.63999938964844, Cash left: 6847.969985961914\n",
      "Day 302: Bought 1 share at 256.55999755859375, Cash left: 6591.40998840332\n",
      "Day 303: Bought 1 share at 248.5, Cash left: 6342.90998840332\n",
      "Day 304: Bought 1 share at 249.22999572753903, Cash left: 6093.679992675781\n",
      "Day 305: Bought 1 share at 239.1999969482422, Cash left: 5854.479995727539\n",
      "Day 307: Sold 1 share at 246.3800048828125, Cash: 6100.860000610352\n",
      "Day 308: Sold 1 share at 215.9900054931641, Cash: 6316.850006103516\n",
      "Day 310: Sold 1 share at 219.8000030517578, Cash: 6536.650009155273\n",
      "Day 312: Sold 1 share at 222.6199951171875, Cash: 6759.270004272461\n",
      "Day 314: Sold 1 share at 216.86000061035156, Cash: 6976.1300048828125\n",
      "Day 315: Sold 1 share at 207.6699981689453, Cash: 7183.800003051758\n",
      "Day 316: Sold 1 share at 198.8800048828125, Cash: 7382.68000793457\n",
      "Day 317: Sold 1 share at 200.63999938964844, Cash: 7583.320007324219\n",
      "Day 318: Sold 1 share at 191.7599945068359, Cash: 7775.080001831055\n",
      "Day 320: Sold 1 share at 200.0, Cash: 7975.080001831055\n",
      "Day 321: Sold 1 share at 197.4900054931641, Cash: 8172.570007324219\n",
      "Day 323: Sold 1 share at 201.3800048828125, Cash: 8373.950012207031\n",
      "Day 325: Sold 1 share at 216.1199951171875, Cash: 8590.070007324219\n",
      "Day 327: Bought 1 share at 221.1000061035156, Cash left: 8368.970001220703\n",
      "Day 328: Bought 1 share at 223.27000427246097, Cash left: 8145.699996948242\n",
      "Day 329: Bought 1 share at 210.66000366210935, Cash left: 7935.039993286133\n",
      "Day 331: Bought 1 share at 213.2100067138672, Cash left: 7721.829986572266\n",
      "Day 332: Sold 1 share at 209.2100067138672, Cash: 7931.039993286133\n",
      "Day 333: Sold 1 share at 205.75, Cash: 8136.789993286133\n",
      "Day 334: Sold 1 share at 206.27999877929688, Cash: 8343.06999206543\n",
      "Day 336: Sold 1 share at 210.6000061035156, Cash: 8553.669998168945\n",
      "Day 339: Bought 1 share at 210.72999572753903, Cash left: 8342.940002441406\n",
      "Day 340: Sold 1 share at 216.27000427246097, Cash: 8559.210006713867\n",
      "Day 342: Sold 1 share at 228.1300048828125, Cash: 8787.34001159668\n",
      "Day 343: Bought 1 share at 229.80999755859375, Cash left: 8557.530014038086\n",
      "Day 344: Bought 1 share at 230.2899932861328, Cash left: 8327.240020751953\n",
      "Day 345: Bought 1 share at 226.77999877929688, Cash left: 8100.460021972656\n",
      "Day 347: Bought 1 share at 227.1999969482422, Cash left: 7873.260025024414\n",
      "Day 349: Bought 1 share at 238.25, Cash left: 7635.010025024414\n",
      "Day 350: Bought 1 share at 250.0, Cash left: 7385.010025024414\n",
      "Day 351: Bought 1 share at 254.27000427246097, Cash left: 7130.740020751953\n",
      "Day 352: Bought 1 share at 257.0199890136719, Cash left: 6873.720031738281\n",
      "Day 353: Bought 1 share at 254.22000122070312, Cash left: 6619.500030517578\n",
      "Day 355: Bought 1 share at 261.6300048828125, Cash left: 6357.870025634766\n",
      "Day 356: Bought 1 share at 258.0199890136719, Cash left: 6099.850036621094\n",
      "Day 357: Bought 1 share at 249.02000427246097, Cash left: 5850.830032348633\n",
      "Day 358: Sold 1 share at 240.66000366210935, Cash: 6091.490036010742\n",
      "Day 360: Sold 1 share at 240.8300018310547, Cash: 6332.320037841797\n",
      "Day 361: Sold 1 share at 244.5, Cash: 6576.820037841797\n",
      "Day 362: Sold 1 share at 241.0500030517578, Cash: 6817.870040893555\n",
      "Day 363: Sold 1 share at 238.77000427246097, Cash: 7056.640045166016\n",
      "Day 364: Sold 1 share at 219.52259826660156, Cash: 7276.162643432617\n",
      "Total cash invested: 10000\n",
      "Stock TSLA\n",
      "Final Portfolio Value: 9032.34342956543\n",
      "Cash: 7276.162643432617, Shares held: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    Stock Name  Day Action         Cash Shares Held  Portfolio Value  \\\n",
       " 0         TSLA    0   Hold        10000           0     10000.000000   \n",
       " 1         TSLA    1   Hold        10000           0     10000.000000   \n",
       " 2         TSLA    2   Hold        10000           0     10000.000000   \n",
       " 3         TSLA    3   Hold        10000           0     10000.000000   \n",
       " 4         TSLA    4   Hold        10000           0     10000.000000   \n",
       " ..         ...  ...    ...          ...         ...              ...   \n",
       " 360       TSLA  360   Sell  6332.320038          12      9222.280060   \n",
       " 361       TSLA  361   Sell  6576.820038          11      9266.320038   \n",
       " 362       TSLA  362   Sell  6817.870041          10      9228.370071   \n",
       " 363       TSLA  363   Sell  7056.640045           9      9205.570084   \n",
       " 364       TSLA  364   Sell  7276.162643           8      9032.343430   \n",
       " \n",
       "      Stock Price       Date  \n",
       " 0     160.309998 2023-05-02  \n",
       " 1     160.610001 2023-05-03  \n",
       " 2     161.199997 2023-05-04  \n",
       " 3     170.059998 2023-05-05  \n",
       " 4     171.789993 2023-05-08  \n",
       " ..           ...        ...  \n",
       " 360   240.830002 2024-10-07  \n",
       " 361   244.500000 2024-10-08  \n",
       " 362   241.050003 2024-10-09  \n",
       " 363   238.770004 2024-10-10  \n",
       " 364   219.522598 2024-10-14  \n",
       " \n",
       " [365 rows x 8 columns],\n",
       " 7276.162643432617)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specific_model = joblib.load('models/TSLA_model.pkl')\n",
    "# specific_model = train_Optimal_Action(symbol='MSFT', action_column='Action')\n",
    "specificdf = stock_market_simulation(specific_model, 10000, 365, get_stock_data('TSLA').tail(365), print_results=True)\n",
    "specificdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock Name</th>\n",
       "      <th>Day</th>\n",
       "      <th>Action</th>\n",
       "      <th>Cash</th>\n",
       "      <th>Shares Held</th>\n",
       "      <th>Portfolio Value</th>\n",
       "      <th>Stock Price</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>0</td>\n",
       "      <td>Buy</td>\n",
       "      <td>9839.690002</td>\n",
       "      <td>1</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>160.309998</td>\n",
       "      <td>2023-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>1</td>\n",
       "      <td>Buy</td>\n",
       "      <td>9679.080002</td>\n",
       "      <td>2</td>\n",
       "      <td>10000.300003</td>\n",
       "      <td>160.610001</td>\n",
       "      <td>2023-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2</td>\n",
       "      <td>Buy</td>\n",
       "      <td>9517.880005</td>\n",
       "      <td>3</td>\n",
       "      <td>10001.479996</td>\n",
       "      <td>161.199997</td>\n",
       "      <td>2023-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>3</td>\n",
       "      <td>Hold</td>\n",
       "      <td>9517.880005</td>\n",
       "      <td>3</td>\n",
       "      <td>10028.059998</td>\n",
       "      <td>170.059998</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>4</td>\n",
       "      <td>Hold</td>\n",
       "      <td>9517.880005</td>\n",
       "      <td>3</td>\n",
       "      <td>10033.249985</td>\n",
       "      <td>171.789993</td>\n",
       "      <td>2023-05-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>360</td>\n",
       "      <td>Sell</td>\n",
       "      <td>240.830002</td>\n",
       "      <td>48.330201</td>\n",
       "      <td>11880.192349</td>\n",
       "      <td>240.830002</td>\n",
       "      <td>2024-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>361</td>\n",
       "      <td>Hold</td>\n",
       "      <td>240.830002</td>\n",
       "      <td>48.330201</td>\n",
       "      <td>12057.564098</td>\n",
       "      <td>244.500000</td>\n",
       "      <td>2024-10-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>362</td>\n",
       "      <td>Sell</td>\n",
       "      <td>481.880005</td>\n",
       "      <td>47.330201</td>\n",
       "      <td>11890.825052</td>\n",
       "      <td>241.050003</td>\n",
       "      <td>2024-10-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>363</td>\n",
       "      <td>Sell</td>\n",
       "      <td>720.650009</td>\n",
       "      <td>46.330201</td>\n",
       "      <td>11782.912252</td>\n",
       "      <td>238.770004</td>\n",
       "      <td>2024-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>364</td>\n",
       "      <td>Sell</td>\n",
       "      <td>940.310013</td>\n",
       "      <td>45.330201</td>\n",
       "      <td>10897.542087</td>\n",
       "      <td>219.660004</td>\n",
       "      <td>2024-10-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Stock Name  Day Action         Cash Shares Held  Portfolio Value  \\\n",
       "0         TSLA    0    Buy  9839.690002           1     10000.000000   \n",
       "1         TSLA    1    Buy  9679.080002           2     10000.300003   \n",
       "2         TSLA    2    Buy  9517.880005           3     10001.479996   \n",
       "3         TSLA    3   Hold  9517.880005           3     10028.059998   \n",
       "4         TSLA    4   Hold  9517.880005           3     10033.249985   \n",
       "..         ...  ...    ...          ...         ...              ...   \n",
       "360       TSLA  360   Sell   240.830002   48.330201     11880.192349   \n",
       "361       TSLA  361   Hold   240.830002   48.330201     12057.564098   \n",
       "362       TSLA  362   Sell   481.880005   47.330201     11890.825052   \n",
       "363       TSLA  363   Sell   720.650009   46.330201     11782.912252   \n",
       "364       TSLA  364   Sell   940.310013   45.330201     10897.542087   \n",
       "\n",
       "     Stock Price       Date  \n",
       "0     160.309998 2023-05-02  \n",
       "1     160.610001 2023-05-03  \n",
       "2     161.199997 2023-05-04  \n",
       "3     170.059998 2023-05-05  \n",
       "4     171.789993 2023-05-08  \n",
       "..           ...        ...  \n",
       "360   240.830002 2024-10-07  \n",
       "361   244.500000 2024-10-08  \n",
       "362   241.050003 2024-10-09  \n",
       "363   238.770004 2024-10-10  \n",
       "364   219.660004 2024-10-14  \n",
       "\n",
       "[365 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simResults['Logistic Regression']\n",
    "simResults['Random Forest']\n",
    "simResults['Stacking']\n",
    "# simResults['SVM']\n",
    "# simResults['XGBoost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MODELCOMPARISON = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'XGBoost', 'Specific Model','Stacking',\n",
    "              'Logistic Regression', 'Random Forest', 'SVM', 'XGBoost', 'Specific Model','Stacking',\n",
    "              'Logistic Regression', 'Random Forest', 'SVM', 'XGBoost', 'Specific Model','Stacking',\n",
    "              'Logistic Regression', 'Random Forest', 'SVM', 'XGBoost', 'Specific Model','Stacking',\n",
    "              'Logistic Regression', 'Random Forest', 'SVM', 'XGBoost', 'Specific Model','Stacking',\n",
    "              'Logistic Regression', 'Random Forest', 'SVM', 'XGBoost', 'Specific Model','Stacking',],\n",
    "    'Stock': ['AAPL', 'AAPL', 'AAPL', 'AAPL', 'AAPL','AAPL',\n",
    "              'MSFT', 'MSFT', 'MSFT', 'MSFT', 'MSFT', 'MSFT',\n",
    "              'INTC', 'INTC', 'INTC', 'INTC', 'INTC', 'INTC',\n",
    "              'GOOG', 'GOOG', 'GOOG', 'GOOG', 'GOOG', 'GOOG',\n",
    "              'AMD', 'AMD', 'AMD', 'AMD', 'AMD', 'AMD',\n",
    "              'TSLA', 'TSLA', 'TSLA', 'TSLA', 'TSLA', 'TSLA'],\n",
    "    'Final Portfolio Value': [12010.10, 12438.13, 10991.26, 12404.69, 12404.69, 12560.38,\n",
    "                              11677.37, 11187.02, 11803.34, 11808.35, 11774.20, 10278.36,\n",
    "                              9921.06, 9981.07, 9822.93, 9981.07, 9252.78,9980.80,\n",
    "                              9969.69, 9944.17, 9964.88, 9956.80, 11123.06, 9986.28,\n",
    "                              10187.84, 9921.48, 10446.35, 9921.48, 10742.80,9920.74,\n",
    "                              9144.12, 10501.23, 9193.88, 10614.61, 9016.84,10897.54],\n",
    "    'Total Profit %': [20.01, 24.38, 9.91, 24.04, 24.04, 25.60,\n",
    "                       16.77, 11.87, 18.03, 18.08, 17.74, 2.78,\n",
    "                       -0.79, 0.81, -1.77, 0.81, -7.47, -0.20,\n",
    "                       -0.30, -0.56, -0.35, -0.44, 11.23, -0.14,\n",
    "                       1.19, -0.79, 4.46, -0.79, 7.43, -0.80,\n",
    "                       -8.56, 5.01, -8.06, 6.14, -9.83, 8.98]\n",
    "}).sort_values('Final Portfolio Value')\n",
    "MODELCOMPARISON.to_csv('data/MODELCOMPARISON.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Specific Model",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364
         ],
         "xaxis": "x",
         "y": [
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          10000,
          9999.109985351562,
          10003.75,
          10017.66000366211,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10002.669982910156,
          10003.849975585938,
          10003.849975585938,
          10015.329971313477,
          10010.759979248047,
          10009.899993896484,
          10006.90998840332,
          10006.90998840332,
          10006.90998840332,
          10010.729965209961,
          10015.469985961914,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.81999206543,
          10013.189987182617,
          10004.949996948242,
          9981.699996948242,
          9987.300003051758,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.230010986328,
          9988.160018920898,
          9963.900009155273,
          9977.91000366211,
          10014.15005493164,
          10068.95004272461,
          10090.670013427734,
          10053.670013427734,
          10057.22004699707,
          10065.020065307617,
          10104.220001220703,
          10048.300079345703,
          10059.459991455078,
          10065.760040283203,
          10182.800033569336,
          10154.420013427734,
          10105.70004272461,
          10089.45004272461,
          10043.95004272461,
          10091.050033569336,
          10100.149948120117,
          10142.66000366211,
          10157.059967041016,
          10111.960067749023,
          10084.659957885742,
          10105.179946899414,
          10199.260025024414,
          10216.41000366211,
          10205.050018310547,
          10251.310012817383,
          10160.58999633789,
          10219.470001220703,
          10205.749954223633,
          10230.169906616211,
          10263.980026245117,
          10206.159957885742,
          10177.959976196289,
          10177.65998840332,
          10137.779983520508,
          10136.219970703125,
          10135.33999633789,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10138.299987792969,
          10132.859985351562,
          10124.63998413086,
          10138.709991455078,
          10185.669952392578,
          10183.669952392578,
          10152.719940185547,
          10158.779998779297,
          10177.259994506836,
          10133.739974975586,
          10200.609909057617,
          10203.909927368164,
          10229.319900512695,
          10227.400039672852,
          10237.279968261719,
          10034.279968261719,
          9923.280059814453,
          9856.079864501953,
          9887.729873657227,
          9838.07991027832,
          9872.100021362305,
          9868.879867553711,
          9764.099899291992,
          9680.339950561523,
          9692.11003112793,
          9794.40998840332,
          9769.61003112793,
          9808.669998168945,
          9785.95002746582,
          9768.039978027344,
          9782.440002441406,
          9817.719955444336,
          9832.839981079102,
          9800.519912719727,
          9795.38998413086,
          9709.490020751953,
          9728.740020751953,
          9756.039978027344,
          9700.149917602539,
          9780.949935913086,
          9819.95002746582,
          9773.869934082031,
          9802.270050048828,
          9770.320022583008,
          9674.61994934082,
          9635.289993286133,
          9622.009963989258,
          9583.369934082031,
          9566.08999633789,
          9541.08999633789,
          9551.609954833984,
          9603.959991455078,
          9628.109954833984,
          9622.439956665039,
          9725.479995727539,
          9682.399978637695,
          9665.950012207031,
          9666.069946289062,
          9674.330001831055,
          9702.889938354492,
          9640.339965820312,
          9609.440002441406,
          9579.190002441406,
          9537.190002441406,
          9581.649978637695,
          9655.230026245117,
          9612.510055541992,
          9623.559936523438,
          9657.620071411133,
          9627.499954223633,
          9755.65005493164,
          9684.259994506836,
          9607.820053100586,
          9668.320053100586,
          9640.929992675781,
          9634.210021972656,
          9662.809921264648,
          9655.710006713867,
          9639.59992980957,
          9627.440017700195,
          9629.509979248047,
          9653.029998779297,
          9649.809951782227,
          9627.669937133789,
          9612.019989013672,
          9638.539947509766,
          9654.079971313477,
          9645.159957885742,
          9654.589965820312,
          9654.589965820312,
          9651.29997253418,
          9651.29997253418,
          9650.869979858398,
          9650.869979858398,
          9650.869979858398,
          9651.919982910156,
          9652.83999633789,
          9688.779983520508,
          9774.379959106445,
          9850.029983520508,
          9880.810012817383,
          9890.75,
          9965.869873046875,
          9974.24008178711,
          9751.939971923828,
          9831.139938354492,
          9879.649978637695,
          9926.689956665039,
          9821.90998840332,
          9832.129928588867,
          9681.679946899414,
          9878.639907836914,
          9796.560073852539,
          9340.710083007812,
          9400.350006103516,
          9394.050048828125,
          9553.950088500977,
          9430.709945678711,
          9544.110092163086,
          9361.59001159668,
          9260.499984741211,
          9172.600051879883,
          9188.440002441406,
          9117.399963378906,
          9166.959976196289,
          9175.080001831055,
          9160.020034790039,
          9211.720016479492,
          9179.470031738281,
          9230.510009765625,
          9238.429992675781,
          9258.230010986328,
          9253.370025634766,
          9262.050018310547,
          9199.000015258789,
          9256.960037231445,
          9214.300033569336,
          9186.300033569336,
          9165.539993286133,
          9168.189987182617,
          9199.509994506836,
          9185.470016479492,
          9211.900009155273,
          9244.179992675781,
          9185.859985351562,
          9208.02001953125,
          9237.720001220703,
          9243.600021362305,
          9246.960006713867,
          9248.399993896484,
          9234.36001586914,
          9239.809997558594,
          9236.460006713867,
          9336.780014038086,
          9302.760025024414,
          9385.010025024414,
          9419.170059204102,
          9443.919921875,
          9415.920043945312,
          9484.559936523438,
          9497.430084228516,
          9454.109893798828,
          9337.110092163086,
          9220.070083618164,
          9342.530059814453,
          9222.280059814453,
          9266.320037841797,
          9228.370071411133,
          9205.570083618164,
          9016.840072631836
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines",
         "name": "Stock Price",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364
         ],
         "xaxis": "x2",
         "y": [
          160.30999755859375,
          160.61000061035156,
          161.1999969482422,
          170.05999755859375,
          171.7899932861328,
          169.14999389648438,
          168.5399932861328,
          172.0800018310547,
          167.97999572753906,
          166.35000610351562,
          166.52000427246094,
          173.86000061035156,
          176.88999938964844,
          180.13999938964844,
          188.8699951171875,
          185.77000427246097,
          182.8999938964844,
          184.47000122070312,
          193.1699981689453,
          201.16000366210935,
          203.92999267578125,
          207.52000427246097,
          213.97000122070312,
          217.61000061035156,
          221.30999755859375,
          224.57000732421875,
          234.86000061035156,
          244.3999938964844,
          249.8300018310547,
          258.7099914550781,
          256.7900085449219,
          255.8999938964844,
          260.5400085449219,
          274.45001220703125,
          259.4599914550781,
          264.6099853515625,
          256.6000061035156,
          241.0500030517578,
          250.2100067138672,
          256.239990234375,
          257.5,
          261.7699890136719,
          279.82000732421875,
          282.4800109863281,
          276.5400085449219,
          274.42999267578125,
          269.6099853515625,
          269.7900085449219,
          271.989990234375,
          277.8999938964844,
          281.3800048828125,
          290.3800048828125,
          293.3399963378906,
          291.260009765625,
          262.8999938964844,
          260.0199890136719,
          269.05999755859375,
          265.2799987792969,
          264.3500061035156,
          255.7100067138672,
          266.44000244140625,
          267.42999267578125,
          261.07000732421875,
          254.11000061035156,
          259.32000732421875,
          253.86000061035156,
          251.4499969482422,
          249.6999969482422,
          242.19000244140625,
          245.33999633789065,
          242.6499938964844,
          239.7599945068359,
          232.9600067138672,
          225.6000061035156,
          219.22000122070312,
          215.4900054931641,
          231.27999877929688,
          233.19000244140625,
          236.86000061035156,
          230.0399932861328,
          238.58999633789065,
          238.82000732421875,
          257.17999267578125,
          256.8999938964844,
          258.0799865722656,
          245.0099945068359,
          256.489990234375,
          251.9199981689453,
          251.4900054931641,
          248.5,
          273.5799865722656,
          267.4800109863281,
          271.29998779296875,
          276.0400085449219,
          274.3900146484375,
          265.2799987792969,
          266.5,
          262.5899963378906,
          255.6999969482422,
          244.8800048828125,
          246.9900054931641,
          244.1199951171875,
          240.5,
          246.3800048828125,
          250.22000122070312,
          251.6000061035156,
          246.52999877929688,
          261.1600036621094,
          260.04998779296875,
          260.5299987792969,
          259.6700134277344,
          263.6199951171875,
          262.989990234375,
          258.8699951171875,
          251.1199951171875,
          253.9199981689453,
          254.8500061035156,
          242.67999267578125,
          220.11000061035156,
          211.9900054931641,
          212.0800018310547,
          216.52000427246097,
          212.4199981689453,
          205.7599945068359,
          207.3000030517578,
          197.36000061035156,
          200.83999633789065,
          205.66000366210935,
          218.5099945068359,
          219.9600067138672,
          219.27000427246097,
          222.17999267578125,
          222.11000061035156,
          209.97999572753903,
          214.6499938964844,
          223.7100067138672,
          237.41000366210935,
          242.83999633789065,
          233.58999633789065,
          234.3000030517578,
          235.6000061035156,
          241.1999969482422,
          234.2100067138672,
          235.4499969482422,
          236.0800018310547,
          246.72000122070312,
          244.13999938964844,
          240.0800018310547,
          238.8300018310547,
          235.5800018310547,
          238.72000122070312,
          239.3699951171875,
          242.63999938964844,
          243.83999633789065,
          239.7400054931641,
          237.0099945068359,
          239.2899932861328,
          251.0500030517578,
          253.5,
          252.0800018310547,
          257.2200012207031,
          247.13999938964844,
          254.5,
          252.5399932861328,
          256.6099853515625,
          261.44000244140625,
          253.17999267578125,
          248.47999572753903,
          248.4199981689453,
          238.4499969482422,
          237.92999267578125,
          237.4900054931641,
          240.4499969482422,
          234.9600067138672,
          233.94000244140625,
          227.22000122070312,
          218.88999938964844,
          219.91000366210935,
          215.5500030517578,
          211.8800048828125,
          212.19000244140625,
          208.8000030517578,
          209.13999938964844,
          207.8300018310547,
          182.6300048828125,
          183.25,
          190.92999267578125,
          191.58999633789065,
          187.2899932861328,
          188.86000061035156,
          187.91000366210935,
          181.05999755859375,
          185.1000061035156,
          187.5800018310547,
          189.55999755859375,
          193.57000732421875,
          188.1300048828125,
          184.02000427246097,
          188.7100067138672,
          200.4499969482422,
          199.9499969482422,
          193.7599945068359,
          194.77000427246097,
          197.41000366210935,
          191.97000122070312,
          199.3999938964844,
          199.72999572753903,
          202.0399932861328,
          201.8800048828125,
          202.63999938964844,
          188.13999938964844,
          180.7400054931641,
          176.5399932861328,
          178.64999389648438,
          175.33999633789062,
          177.77000427246094,
          177.5399932861328,
          169.47999572753906,
          162.5,
          163.57000732421875,
          173.8000030517578,
          171.32000732421875,
          175.66000366210938,
          172.82000732421875,
          170.8300018310547,
          172.6300048828125,
          177.6699981689453,
          179.8300018310547,
          175.7899932861328,
          175.22000122070312,
          166.6300048828125,
          168.3800048828125,
          171.11000061035156,
          164.89999389648438,
          172.97999572753906,
          176.8800048828125,
          171.75999450683594,
          174.60000610351562,
          171.0500030517578,
          161.47999572753906,
          157.11000061035156,
          155.4499969482422,
          149.92999267578125,
          147.0500030517578,
          142.0500030517578,
          144.67999267578125,
          162.1300048828125,
          170.17999267578125,
          168.2899932861328,
          194.0500030517578,
          183.27999877929688,
          179.99000549316406,
          180.00999450683594,
          181.19000244140625,
          184.7599945068359,
          177.80999755859375,
          174.72000122070312,
          171.97000122070312,
          168.47000122070312,
          171.88999938964844,
          177.5500030517578,
          173.99000549316406,
          174.83999633789062,
          177.4600067138672,
          174.9499969482422,
          186.6000061035156,
          180.11000061035156,
          173.74000549316406,
          179.24000549316406,
          176.75,
          176.19000244140625,
          178.7899932861328,
          178.0800018310547,
          176.2899932861328,
          174.77000427246094,
          175,
          177.94000244140625,
          177.47999572753906,
          173.7899932861328,
          170.66000366210938,
          177.2899932861328,
          182.47000122070312,
          178.00999450683594,
          187.44000244140625,
          184.86000061035156,
          181.57000732421875,
          183.0099945068359,
          182.5800018310547,
          187.3500061035156,
          196.3699951171875,
          197.4199981689453,
          197.8800048828125,
          209.86000061035156,
          231.2599945068359,
          246.38999938964844,
          251.52000427246097,
          252.94000244140625,
          262.3299865722656,
          263.260009765625,
          241.02999877929688,
          248.22999572753903,
          252.63999938964844,
          256.55999755859375,
          248.5,
          249.22999572753903,
          239.1999969482422,
          251.5099945068359,
          246.3800048828125,
          215.9900054931641,
          220.25,
          219.8000030517578,
          232.1000061035156,
          222.6199951171875,
          232.07000732421875,
          216.86000061035156,
          207.6699981689453,
          198.8800048828125,
          200.63999938964844,
          191.7599945068359,
          198.83999633789065,
          200,
          197.4900054931641,
          207.8300018310547,
          201.3800048828125,
          214.13999938964844,
          216.1199951171875,
          222.72000122070312,
          221.1000061035156,
          223.27000427246097,
          210.66000366210935,
          220.32000732421875,
          213.2100067138672,
          209.2100067138672,
          205.75,
          206.27999877929688,
          214.11000061035156,
          210.6000061035156,
          219.41000366210935,
          230.1699981689453,
          210.72999572753903,
          216.27000427246097,
          226.1699981689453,
          228.1300048828125,
          229.80999755859375,
          230.2899932861328,
          226.77999877929688,
          227.8699951171875,
          227.1999969482422,
          243.9199981689453,
          238.25,
          250,
          254.27000427246097,
          257.0199890136719,
          254.22000122070312,
          260.4599914550781,
          261.6300048828125,
          258.0199890136719,
          249.02000427246097,
          240.66000366210935,
          250.0800018310547,
          240.8300018310547,
          244.5,
          241.0500030517578,
          238.77000427246097,
          217.8000030517578
         ],
         "yaxis": "y2"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Final Portfolio Value",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Stock Price",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.375,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Portfolio Analysis"
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "matches": "x2",
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.625,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          0.375
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create subplots with 2 rows and 1 column\n",
    "fig = sp.make_subplots(rows=2, cols=1, subplot_titles=('Final Portfolio Value', 'Stock Price'), shared_xaxes=True)\n",
    "\n",
    "# Add trace for Portfolio Value in the first row\n",
    "fig.add_trace(go.Scatter(x=specificdf[0]['Day'], y=specificdf[0]['Portfolio Value'],mode='lines', name='Specific Model'), row=1, col=1)\n",
    "\n",
    "# Add trace for Stock Price in the first row\n",
    "fig.add_trace(go.Scatter(x=specificdf[0]['Day'], y=specificdf[0]['Stock Price'], mode='lines', name='Stock Price'), row=2, col=1)\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(height=600, width=800, title_text=\"Portfolio Analysis\")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock Name</th>\n",
       "      <th>Day</th>\n",
       "      <th>Action</th>\n",
       "      <th>Cash</th>\n",
       "      <th>Shares Held</th>\n",
       "      <th>Portfolio Value</th>\n",
       "      <th>Stock Price</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>0</td>\n",
       "      <td>Hold</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>160.309998</td>\n",
       "      <td>2023-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>1</td>\n",
       "      <td>Hold</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>160.610001</td>\n",
       "      <td>2023-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2</td>\n",
       "      <td>Hold</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>161.199997</td>\n",
       "      <td>2023-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>3</td>\n",
       "      <td>Hold</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>170.059998</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>4</td>\n",
       "      <td>Hold</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>171.789993</td>\n",
       "      <td>2023-05-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>360</td>\n",
       "      <td>Sell</td>\n",
       "      <td>6332.320038</td>\n",
       "      <td>12</td>\n",
       "      <td>9222.280060</td>\n",
       "      <td>240.830002</td>\n",
       "      <td>2024-10-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>361</td>\n",
       "      <td>Sell</td>\n",
       "      <td>6576.820038</td>\n",
       "      <td>11</td>\n",
       "      <td>9266.320038</td>\n",
       "      <td>244.500000</td>\n",
       "      <td>2024-10-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>362</td>\n",
       "      <td>Sell</td>\n",
       "      <td>6817.870041</td>\n",
       "      <td>10</td>\n",
       "      <td>9228.370071</td>\n",
       "      <td>241.050003</td>\n",
       "      <td>2024-10-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>363</td>\n",
       "      <td>Sell</td>\n",
       "      <td>7056.640045</td>\n",
       "      <td>9</td>\n",
       "      <td>9205.570084</td>\n",
       "      <td>238.770004</td>\n",
       "      <td>2024-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>364</td>\n",
       "      <td>Sell</td>\n",
       "      <td>7274.440048</td>\n",
       "      <td>8</td>\n",
       "      <td>9016.840073</td>\n",
       "      <td>217.800003</td>\n",
       "      <td>2024-10-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Stock Name  Day Action         Cash Shares Held  Portfolio Value  \\\n",
       "0         TSLA    0   Hold        10000           0     10000.000000   \n",
       "1         TSLA    1   Hold        10000           0     10000.000000   \n",
       "2         TSLA    2   Hold        10000           0     10000.000000   \n",
       "3         TSLA    3   Hold        10000           0     10000.000000   \n",
       "4         TSLA    4   Hold        10000           0     10000.000000   \n",
       "..         ...  ...    ...          ...         ...              ...   \n",
       "360       TSLA  360   Sell  6332.320038          12      9222.280060   \n",
       "361       TSLA  361   Sell  6576.820038          11      9266.320038   \n",
       "362       TSLA  362   Sell  6817.870041          10      9228.370071   \n",
       "363       TSLA  363   Sell  7056.640045           9      9205.570084   \n",
       "364       TSLA  364   Sell  7274.440048           8      9016.840073   \n",
       "\n",
       "     Stock Price       Date  \n",
       "0     160.309998 2023-05-02  \n",
       "1     160.610001 2023-05-03  \n",
       "2     161.199997 2023-05-04  \n",
       "3     170.059998 2023-05-05  \n",
       "4     171.789993 2023-05-08  \n",
       "..           ...        ...  \n",
       "360   240.830002 2024-10-07  \n",
       "361   244.500000 2024-10-08  \n",
       "362   241.050003 2024-10-09  \n",
       "363   238.770004 2024-10-10  \n",
       "364   217.800003 2024-10-11  \n",
       "\n",
       "[365 rows x 8 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificdf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:237: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AAPL_model_Stacking_Sector.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stock_df = get_stock_data('TSLA')\n",
    "# print(f'Adding columns...')\n",
    "stock_df = pd.read_csv('data/sp500_stocks_sector.csv')\n",
    "stock_df = stock_df[stock_df['Symbol'] == 'AAPL']\n",
    "print(f'Scaling data...')\n",
    "preprocessed,features = scale_data(stock_df)\n",
    "X = preprocessed[features]\n",
    "y = stock_df['Action']\n",
    "# y = y.map({'Buy': 0, 'Sell': 1, 'Hold': 2})\n",
    "X_train, _, y_train, _ = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('xgb', xgb.XGBClassifier(n_estimators=100, random_state=42)),\n",
    "        # (\"specificModel\", joblib.load('models/MSFT_model.pkl'))\n",
    "    ],\n",
    "    final_estimator=xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Volume', 'MA_10', 'MA_20', 'MA_50', 'MA_200', 'std_10', 'std_20',\n",
       "       'std_50', 'std_200', 'upper_band_10', 'lower_band_10', 'upper_band_20',\n",
       "       'lower_band_20', 'upper_band_50', 'lower_band_50', 'upper_band_200',\n",
       "       'lower_band_200', 'Golden_Cross_Short', 'Golden_Cross_Medium',\n",
       "       'Golden_Cross_Long', 'Death_Cross_Short', 'Death_Cross_Medium',\n",
       "       'Death_Cross_Long', 'ROC', 'AVG_Volume_10', 'AVG_Volume_20',\n",
       "       'AVG_Volume_50', 'AVG_Volume_200', 'Doji', 'Bullish_Engulfing',\n",
       "       'Bearish_Engulfing', 'MACD', 'Signal', 'MACD_Hist', 'TR', 'ATR',\n",
       "       'RSI_10_Day', '10_Day_ROC', 'Resistance_10_Day', 'Support_10_Day',\n",
       "       'Resistance_20_Day', 'Support_20_Day', 'Resistance_50_Day',\n",
       "       'Support_50_Day', 'Volume_MA_10', 'Volume_MA_20', 'Volume_MA_50', 'OBV',\n",
       "       'Z-score', 'Basic Materials', 'Communication Services',\n",
       "       'Consumer Cyclical', 'Consumer Defensive', 'Energy',\n",
       "       'Financial Services', 'Healthcare', 'Industrials', 'Real Estate',\n",
       "       'Technology', 'Utilities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stock_df = pd.read_csv('data/sp500_stocks_sector.csv')\n",
    "# sector = pd.get_dummies(stock_df['Sector']).astype(int)\n",
    "# stock_df = pd.concat([stock_df, sector], axis=1)\n",
    "# stock_df.to_csv('data/sp500_stocks_sector.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardobenjamin/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:237: FutureWarning: The behavior of DataFrame.idxmax with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  '10_Day_Return', '20_Day_Return', '50_Day_Return', '200_Day_Return']].idxmax(axis=1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Basic Materials\n- Communication Services\n- Consumer Cyclical\n- Consumer Defensive\n- Energy\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL_model_Stacking_Sector.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m stock_market_simulation(model, \u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m365\u001b[39m, get_stock_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtail(\u001b[38;5;241m365\u001b[39m), print_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:82\u001b[0m, in \u001b[0;36mstock_market_simulation\u001b[0;34m(model, initial_cash, days, stock, existing_shares, oneDay, print_results)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(days):\n\u001b[1;32m     81\u001b[0m     stock_price \u001b[38;5;241m=\u001b[39m stock[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i]\n\u001b[0;32m---> 82\u001b[0m     strategy \u001b[38;5;241m=\u001b[39m predict_action(scaled\u001b[38;5;241m.\u001b[39miloc[i]\u001b[38;5;241m.\u001b[39mto_dict(), model)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m oneDay:\n\u001b[1;32m     84\u001b[0m         day \u001b[38;5;241m=\u001b[39m oneDay\n",
      "File \u001b[0;32m~/Desktop/Repos/MachineLearningProjects/Stock-Market-Analysis+Model/SimulateDay.py:58\u001b[0m, in \u001b[0;36mpredict_action\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m     56\u001b[0m     data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([data])\n\u001b[1;32m     57\u001b[0m     data_df \u001b[38;5;241m=\u001b[39m data_df[features]\n\u001b[0;32m---> 58\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(data_df)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuy\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:695\u001b[0m, in \u001b[0;36mStackingClassifier.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;129m@available_if\u001b[39m(_estimator_has(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params):\n\u001b[1;32m    676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict target for X.\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \n\u001b[1;32m    678\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m        Predicted targets.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# Handle the multilabel-indicator case\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    699\u001b[0m             [\n\u001b[1;32m    700\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder[target_idx]\u001b[38;5;241m.\u001b[39minverse_transform(target)\n\u001b[1;32m    701\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m target_idx, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    702\u001b[0m             ]\n\u001b[1;32m    703\u001b[0m         )\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:382\u001b[0m, in \u001b[0;36m_BaseStacking.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict target for X.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    Predicted targets.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    381\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_estimator_\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(X), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:766\u001b[0m, in \u001b[0;36mStackingClassifier.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return class labels or probabilities for X for each estimator.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m        Prediction outputs for each estimator.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:306\u001b[0m, in \u001b[0;36m_BaseStacking._transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Concatenate and return the predictions of the estimators.\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 306\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(est, meth)(X)\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m ]\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concatenate_predictions(X, predictions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:307\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Concatenate and return the predictions of the estimators.\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    306\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(est, meth)(X)\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m ]\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concatenate_predictions(X, predictions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:946\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    944\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 946\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_X_predict(X)\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    949\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:641\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    639\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    642\u001b[0m     X,\n\u001b[1;32m    643\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE,\n\u001b[1;32m    644\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    645\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    646\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[1;32m    647\u001b[0m )\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Basic Materials\n- Communication Services\n- Consumer Cyclical\n- Consumer Defensive\n- Energy\n- ...\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('AAPL_model_Stacking_Sector.pkl')\n",
    "stock_market_simulation(model, 10000, 365, get_stock_data('AAPL').tail(365), print_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
